# -*- coding: utf-8 -*-
"""XGboost

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19lkQHD4khd6oYFhdkzwv1wz8qUA8jNdP
"""

import pandas as pd
import numpy as np
import nltk
import re
import torch
import joblib
from transformers import AutoTokenizer, AutoModel
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertModel

# Acces to google drive for dataset

from google.colab import drive
drive.mount('/content/drive')

# Load Dataset

data = pd.read_csv("/content/drive/MyDrive/university_text_classification_220k.csv")
data.head()

# Check the class distribution and missing values

print(data['category'].value_counts())
print("\n\n\n Missing Values: ",data.isnull().sum())

nltk.download("stopwords")
stop_words = set(stopwords.words('turkish'))



def clean_text(text):
    text = str(text)
    text = text.lower()

    # Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)

    # Remove stopwords using NLTK's Turkish stopwords
    text = ' '.join(word for word in text.split() if word not in stop_words)

    # Remove special characters/emojis (keeps Turkish characters: ç, ğ, ı, ö, ş, ü)
    text = re.sub(r'[^\u00E7\u011F\u0131\u00F6\u015F\u00FC\s\w]', '', text)

    # Remove extra whitespace
    text = ' '.join(text.split())

    return text




data["cleaned_text"] = data["ticket_text"].apply(clean_text)
print(data["cleaned_text"])

# Split the dataset into training %80 and test %20 sets.


def split_data(data, text_column, label_column):

    X = data[text_column]
    y = data[label_column]

    # Split into train (80%) and test (20%)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    print("\nDataset Split Sizes:")
    print(f"Training set: {len(X_train)} samples")
    print(f"Test set: {len(X_test)} samples")

    return X_train, X_test, y_train, y_test



X_train, X_test, y_train, y_test = split_data(data, 'cleaned_text', 'category')

# BERT ile metin vektörleştirme
def vectorize_bert(X_train, X_test, model_name="dbmdz/bert-base-turkish-cased", batch_size=16):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    def get_sentence_embedding(texts):
        inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=128)
        inputs = {key: val.to(device) for key, val in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        return outputs.last_hidden_state[:, 0, :].cpu().numpy()

    def process_in_batches(texts):
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = get_sentence_embedding(batch_texts)
            embeddings.append(batch_embeddings)
        return np.vstack(embeddings)

    X_train_list = X_train.tolist() if isinstance(X_train, (pd.Series, np.ndarray)) else X_train
    X_test_list = X_test.tolist() if isinstance(X_test, (pd.Series, np.ndarray)) else X_test

    X_train_bert = process_in_batches(X_train_list)
    X_test_bert = process_in_batches(X_test_list)

    print("\nBERT Embedding Şekilleri:")
    print(f"Eğitim: {X_train_bert.shape}")
    print(f"Test: {X_test_bert.shape}")

    # Embedding'leri diske kaydet
    np.save("X_train_bert.npy", X_train_bert)
    np.save("X_test_bert.npy", X_test_bert)

    return X_train_bert, X_test_bert, tokenizer, model






X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['category'], test_size=0.2, random_state=42)

# BERT ile embedding oluşturma ve kaydetme
X_train_bert, X_test_bert, tokenizer, model = vectorize_bert(X_train, X_test)

# Convert string labels to numeric using LabelEncoder.
def encode_labels(y_train, y_test):
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_test_encoded = le.transform(y_test)
    print("\nLabel Encoding Classes:", le.classes_)
    return y_train_encoded, y_test_encoded, le






# Train XGBoost with BERT vectors
def train_xgboost(X_train_bert, y_train_encoded, X_val_bert=None, y_val_encoded=None):
    print("\nTraining the XGBoost model with GPU...")
    model = XGBClassifier(
        n_estimators=200,              # Number of boosting rounds (trees)
        max_depth=3,                   # Maximum depth of each tree
        learning_rate=0.05,             # Step size shrinkage to prevent overfitting
        random_state=42,               # Seed for reproducibility
        n_jobs=-1,                     # Use all CPU cores (no effect with GPU)
        tree_method='gpu_hist',        # Use GPU-accelerated histogram method
        predictor='gpu_predictor',     # Use GPU for predictions
        objective='multi:softmax',     # Objective function for multi-class classification
        num_class=22,                  # Number of classes
        subsample=0.8,                 # Fraction of samples used per tree
        colsample_bytree=0.5,          # Fraction of features used per tree
        min_child_weight=3,            # Minimum sum of instance weight in a child node
        gamma=0.2,                     # Minimum loss reduction for a split
        reg_alpha=0.5,                 # L1 regularization term (Lasso)
        reg_lambda=1.5                 # L2 regularization term (Ridge)
    )
    # Train with early stopping if validation data is provided
    if X_val_bert is not None and y_val_encoded is not None:
        model.fit(
            X_train_bert,
            y_train_encoded,
            eval_set=[(X_val_bert, y_val_encoded)],
            early_stopping_rounds=20,      # Stop if no improvement after 20 rounds
            verbose=True
        )
    else:
        model.fit(X_train_bert, y_train_encoded, verbose=True)
    print("Training completed!")
    return model





# Test the model
def test_xgboost(model, X_test_bert, y_test_encoded, label_encoder):
    y_test_pred = model.predict(X_test_bert)
    print("\nTest Set Classification Report:")
    print(classification_report(y_test_encoded, y_test_pred, target_names=label_encoder.classes_))

    return y_test_pred





#------------------------------------------------------------------------------------------------------------


# Convert string labels to numeric using LabelEncoder.
y_train_encoded, y_test_encoded, label_encoder = encode_labels(y_train, y_test)

# train xgboost
xgboost_model = train_xgboost(X_train_bert, y_train_encoded)

# Test the model
y_test_pred = test_xgboost(xgboost_model, X_test_bert, y_test_encoded, label_encoder)

# Modeli ve LabelEncoder'ı kaydetme
def save_model_and_encoder(model, label_encoder, model_path="/content/drive/My Drive/xgboost_bert_model_.pkl"):

    joblib.dump(model, model_path)
    joblib.dump(label_encoder, encoder_path)
    print(f"\nModel kaydedildi: {model_path}")
    print(f"LabelEncoder kaydedildi: {encoder_path}")


#xgboost_model
model = xgboost_model
label_encoder = label_encoder

# Modeli ve LabelEncoder'ı kaydet
save_model_and_encoder(model, label_encoder)

import joblib
import torch
import numpy as np
from transformers import BertTokenizer, BertModel
import os  # Dosyayı silmek için

# Model ve LabelEncoder'ı yükleme
def load_model_and_encoder(model_path="/content/drive/MyDrive/xgboost_bert_model.pkl",
                           encoder_path="/content/drive/MyDrive/label_encoder.pkl"):
    model = joblib.load(model_path)
    label_encoder = joblib.load(encoder_path)
    print(f"Model yüklendi: {model_path}")
    print(f"LabelEncoder yüklendi: {encoder_path}")
    return model, label_encoder

# BERT ile tek metin embedding oluşturup geçici olarak .npy dosyasına kaydet
def save_bert_embedding(text, save_path="user_input_emb.npy", model_name='dbmdz/bert-base-turkish-cased'):
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertModel.from_pretrained(model_name)
    model.eval()

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].numpy()

    np.save(save_path, embeddings)

# Embedding dosyasından tahmin yap
def predict_from_embedding(embedding_path, model, label_encoder):
    embedding = np.load(embedding_path)
    prediction = model.predict(embedding)
    predicted_label = label_encoder.inverse_transform(prediction)[0]
    return predicted_label

# Metni al, embed et, tahmin et, sonra embedding dosyasını sil
if __name__ == "__main__":
    xgboost_model, label_encoder = load_model_and_encoder()

    user_input = input("")

    temp_embedding_path = "user_input_emb.npy"

    try:
        # Geçici embedding oluştur
        save_bert_embedding(user_input, save_path=temp_embedding_path)

        # Tahmin yap
        predicted_class = predict_from_embedding(temp_embedding_path, xgboost_model, label_encoder)

        print(f"Girilen metin '{predicted_class}' sınıfına aittir.")
    finally:
        # Dosyayı sil
        if os.path.exists(temp_embedding_path):
            os.remove(temp_embedding_path)

